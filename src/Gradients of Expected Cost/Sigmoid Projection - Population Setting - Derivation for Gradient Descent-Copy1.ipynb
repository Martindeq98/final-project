{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a83c4315",
   "metadata": {},
   "source": [
    "# Expected Value of Cost function in population setting\n",
    "## Projecting $p$ entries as sigmoids \n",
    "We have \n",
    "\n",
    "$$C(A, P) = \\text{Tr}\\left((B^* - B)\\Sigma_X(B^* - B)\\right),$$\n",
    "\n",
    "where $$B = P^{-1}AP.$$\n",
    "\n",
    "Here, $A$ is lower triangular and $P$ is a doubly stochastic matrix, namely\n",
    "\n",
    "\\begin{align*}\n",
    "0 \\leq &p_{ij} &\\forall\\ i,j = 1, \\cdots, n\\\\\n",
    "\\sum_{j = 1}^n &p_{ij} = 1&\\forall\\ i = 1, \\cdots, n\\\\\n",
    "\\sum_{i=1}^n &p_{ij} = 1 &\\forall\\ j = 1, \\cdots, n\\\\\n",
    "\\end{align*}\n",
    "\n",
    "## Dealing with the constraints\n",
    "### Non-negativity constraints\n",
    "To deal with the constraints that $p$ must be non-negative, we project the entries by applying the sigmoid function of each entry. Note that the range of the sigmoid is $(0, 1)$, which is also what we require for our entries $p$ (we did not explicitly constrain $p$ to be smaller than 1, but the equality constraints combined with the non-negativity constraints make sure that is the case.\n",
    "\n",
    "Hence, \n",
    "\n",
    "$$P = \\sigma(P_{\\sigma}),$$\n",
    "\n",
    "where $\\sigma(\\cdot)$ indicates the element-wise application of the sigmoid function \n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}.$$\n",
    "\n",
    "So, instead of directly estimating $P$, we estimate $P_\\sigma$. The constraints now becomes an equality constrained program\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{j = 1}^n &p_{ij} = 1&\\forall\\ i = 1, \\cdots, n\\\\\n",
    "\\sum_{i=1}^n &p_{ij} = 1 &\\forall\\ j = 1, \\cdots, n\\\\\n",
    "\\end{align*}\n",
    "\n",
    "## Dealing with equality constraints\n",
    "To deal with the $2n$ equality constraints, we use $\\textit{lagrange multipliers}$ $\\lambda_{row, i}$, $i = 1, \\cdots, n$ and $\\lambda_{col, j}$, $j = 1, \\cdots, n$. The dual function now becomes \n",
    "\n",
    "$$\\mathcal{L}(A, P_\\sigma, \\mathbf{\\lambda}) = C(A, P) - \\sum_{i=1}^n \\lambda_{row, i} \\left(\\sum_{j = 1}^n p_{ij} - 1\\right) - \\sum_{j = 1}^n \\lambda_{col, j} \\left(\\sum_{i = 1}^n p_{ij} - 1\\right),$$\n",
    "where $P = \\sigma(P_\\sigma)$\n",
    "\n",
    "## Optimizing $\\mathcal{L}(A, P_\\sigma, \\mathbf{\\lambda})$\n",
    "let $$q(\\mathbf{\\lambda}) = \\inf_{A, P_{sigma}} \\mathcal{L}(A, P_\\sigma, \\mathbf{\\lambda}).$$\n",
    "Then the $\\textit{dual problem}$ is $$\\max_{\\mathbf{\\lambda}} q(\\mathbf{\\lambda}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98b64fc",
   "metadata": {},
   "source": [
    "## Deriving $q(\\mathbf{\\lambda})$\n",
    "To minimize $\\mathcal{L}(A, P_\\sigma, \\mathbf{\\lambda})$, we will use gradient descent.\n",
    "\n",
    "The partial derivative with respect to entries in $A$ is simple, \n",
    "\n",
    "$$\\frac{\\partial\\mathcal{L}(A, P_\\sigma, \\mathbf{\\lambda})}{\\partial a_{ij}} = \\frac{\\partial C(A, P)}{\\partial a_{ij}}$$\n",
    "\n",
    "Now, the partial derivative with respect to the entries in $p$ is more involved as it occurs also in exactly two equality constraints, namely in $\\lambda_{row, i}$ and in $\\lambda{col, j}$.\n",
    "\n",
    "$$\\frac{\\partial\\mathcal{L}(A, P_\\sigma, \\mathbf{\\lambda})}{\\partial p_{ij}} = \\frac{\\partial C(A, P)}{\\partial p_{ij}} - \\lambda_{row, i}p_{ij} - \\lambda_{col, j}p_{ij}.$$\n",
    "\n",
    "Now, as $C(A, P)$ is unfortunately yet inevitably non-convex, we cannot expect to find the infimum $q(\\mathbf{\\lambda})$. However, let us consider a local minimum $\\tilde{q}(\\mathbf{\\lambda})$, which we will find by gradient descent. Let us start with an initial matrix $A_0$ (e.g. the non-zero matrix) and an initial unconstrained matrix $P_{0,\\sigma} = Z$, where $Z \\sim \\mathcal{N}(\\mathbf{0}, I_{n^2})$. So, $P_\\sigma$ is close to zero, but there is some perturbation to make sure the matrix is non-singular.\n",
    "\n",
    "For a sufficiently small step size, we will do this gradient descent until we have found a stationary point:\n",
    "$$(A_{t + 1}, P_{t + 1}) = (A_t, P_t) - \\eta\\ \\nabla \\mathcal{L}(A_t, P_t, \\mathbf{\\lambda})$$\n",
    "Note that this is an $\\textit{unconstrained}$ optimization problem now, which should make things more easy. \n",
    "\n",
    "Suppose that after a number of iterations, we have found this local minimum with parameters $\\tilde{A}$, $\\tilde{P}$. Then, we have that $$\\tilde{q}(\\mathbf{\\lambda}) = \\mathcal{L}(\\tilde{A}, \\tilde{P}, \\mathbf{\\lambda}).$$\n",
    "\n",
    "## Deriving the dual problem\n",
    "Now that we have $\\tilde{q}(\\mathbf{\\lambda})$, the solution to the dual problem is\n",
    "$$\\max_{\\lambda} \\tilde{q}(\\mathbf{\\lambda}),$$\n",
    "which is again an unconstrained optimization problem, which we can also optimize by maximizing $\\lambda$'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fdc9fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26a9e055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A*:\n",
      "[[0.3  0.   0.  ]\n",
      " [0.23 0.89 0.  ]\n",
      " [0.85 0.9  0.29]]\n",
      "\n",
      "P*:\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# actual solution\n",
    "n = 3\n",
    "As = np.tril(np.random.rand(n, n))\n",
    "Ps = np.identity(n)\n",
    "Sigma = np.identity(n)\n",
    "\n",
    "print(f\"A*:\\n{np.round(As, 2)}\\n\\nP*:\\n{Ps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "394294d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_cost(A, P, As = As, Ps = Ps):\n",
    "    # base on the distribution of X, no actual data needed.\n",
    "    # we need the covariance of X_t - X_{t-1}.\n",
    "    # Then, the expected cost is the trace of this covariance\n",
    "    P_inv = np.linalg.inv(P)\n",
    "    Ps_inv = np.linalg.inv(Ps)\n",
    "    \n",
    "    B = np.matmul(P_inv, np.matmul(A, P))\n",
    "    Bs = np.matmul(Ps_inv, np.matmul(As, Ps))\n",
    "    \n",
    "    covariance_X = np.matmul(np.linalg.inv(np.identity(n ** 2) - np.kron(Bs, Bs)), Sigma.reshape(n ** 2)).reshape((n, n))\n",
    "    \n",
    "    covariance_matrix = Sigma + np.matmul((Bs - B), np.matmul(covariance_X, (Bs - B).transpose()))\n",
    "    \n",
    "    return np.trace(covariance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8698ce36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected cost: 12.88024614190274.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Expected cost: {expected_cost(np.random.rand(n,n), Ps)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df524e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# get gradient of our outcome\n",
    "def grad_a(A, Psigma, i, j, As = As, Ps = Ps):\n",
    "    P = sigmoid(Psigma)\n",
    "    P_inv = np.linalg.inv(P)\n",
    "    Ps_inv = np.linalg.inv(Ps)\n",
    "    \n",
    "    B = np.matmul(P_inv, np.matmul(A, P))\n",
    "    Bs = np.matmul(Ps_inv, np.matmul(As, Ps))\n",
    "    \n",
    "    J = np.zeros((n, n))\n",
    "    J[i][j] = 1\n",
    "    \n",
    "    covariance_X = np.matmul(np.linalg.inv(np.identity(n ** 2) - np.kron(Bs, Bs)), Sigma.reshape(n ** 2)).reshape((n, n))\n",
    "\n",
    "    return -2 * np.trace(np.matmul(covariance_X, np.matmul((Bs - B).transpose(), np.matmul(P_inv, np.matmul(J, P)))))\n",
    "\n",
    "def grad_p(A, Psigma, i, j, As = As, Ps = Ps):\n",
    "    P = sigmoid(Psigma)\n",
    "    P_inv = np.linalg.inv(P)\n",
    "    Ps_inv = np.linalg.inv(Ps)\n",
    "    \n",
    "    B = np.matmul(P_inv, np.matmul(A, P))\n",
    "    Bs = np.matmul(Ps_inv, np.matmul(As, Ps))\n",
    "    \n",
    "    J = np.zeros((n, n))\n",
    "    J[i][j] = 1\n",
    "    \n",
    "    covariance_X = np.matmul(np.linalg.inv(np.identity(n ** 2) - np.kron(Bs, Bs)), Sigma.reshape(n ** 2)).reshape((n, n))\n",
    "\n",
    "    B_grad = np.matmul(P_inv, np.matmul(A, J))\n",
    "    B_grad -= np.matmul(P_inv, np.matmul(J, np.matmul(P_inv, np.matmul(A, P))))\n",
    "    \n",
    "    return -2 * np.trace(np.matmul(covariance_X, np.matmul((Bs - B).transpose(), B_grad))) * sigmoid(P[i][j]) * (1 - sigmoid(P[i][j]))\n",
    "\n",
    "def get_gradient(A, P):\n",
    "    gradient = np.zeros(int(n * (n + 1) / 2 + n ** 2))\n",
    "    index = 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i + 1):\n",
    "            gradient[index] = grad_a(A, P, i, j)\n",
    "            index += 1\n",
    "            \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            gradient[index] = grad_p(A, P, i, j)\n",
    "            index += 1\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e1d518a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19661193324148185\n",
      "[[ 0.          0.09685158  0.        ]\n",
      " [-0.29737716  0.         -0.85045194]\n",
      " [ 0.          0.40104964  0.        ]]\n",
      "-271.2496227567504\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31bc7296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4083684  0.39949239 0.33551572]\n",
      " [0.55767729 0.51830324 0.66418083]\n",
      " [0.58863107 0.61089676 0.41700066]]\n"
     ]
    }
   ],
   "source": [
    "A = np.zeros((n, n))\n",
    "P = np.ones((n, n)) / n + np.random.rand(n, n) / n\n",
    "print(P)\n",
    "eta = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84504f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57121ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.714501214120054\n",
      "4.69127839812642\n",
      "4.689678020209154\n",
      "4.6880809089565\n",
      "4.686487057420481\n",
      "4.684896458668038\n",
      "4.6833091057809435\n",
      "4.681724991855852\n",
      "4.680144110004187\n",
      "4.678566453352152\n",
      "4.676992015040706\n",
      "4.67542078822549\n",
      "4.673852766076839\n",
      "4.672287941779723\n",
      "4.6707263085337125\n",
      "4.669167859552983\n",
      "4.667612588066237\n",
      "4.666060487316695\n",
      "4.6645115505620725\n",
      "4.66296577107453\n",
      "4.661423142140645\n",
      "4.6598836570614\n",
      "4.6583473091521235\n",
      "4.65681409174247\n",
      "4.655283998176407\n",
      "4.6537570218121465\n",
      "4.652233156022151\n",
      "4.650712394193082\n",
      "4.649194729725773\n",
      "4.647680156035196\n",
      "4.646168666550431\n",
      "4.6446602547146565\n",
      "4.643154913985085\n",
      "4.641652637832951\n",
      "4.640153419743481\n",
      "4.638657253215857\n",
      "4.637164131763198\n",
      "4.635674048912509\n",
      "4.634186998204672\n",
      "4.632702973194406\n",
      "4.63122196745023\n",
      "4.629743974554457\n",
      "4.628268988103123\n",
      "4.626797001706011\n",
      "4.625328008986575\n",
      "4.623862003581927\n",
      "4.622398979142826\n",
      "4.620938929333609\n",
      "4.619481847832196\n",
      "4.6180277283300395\n",
      "4.616576564532117\n",
      "4.615128350156871\n",
      "4.613683078936215\n",
      "4.61224074461547\n",
      "4.61080134095336\n",
      "4.609364861721989\n",
      "4.6079313007067615\n",
      "4.606500651706423\n",
      "4.6050729085329944\n",
      "4.603648065011727\n",
      "4.6022261149811134\n",
      "4.600807052292833\n",
      "4.599390870811735\n",
      "4.597977564415778\n",
      "4.596567126996081\n",
      "4.595159552456793\n",
      "4.593754834715135\n",
      "4.592352967701348\n",
      "4.590953945358676\n",
      "4.589557761643315\n",
      "4.588164410524415\n",
      "4.586773885984014\n",
      "4.585386182017062\n",
      "4.584001292631346\n",
      "4.582619211847476\n",
      "4.581239933698873\n",
      "4.579863452231731\n",
      "4.578489761504974\n",
      "4.577118855590266\n",
      "4.575750728571924\n",
      "4.5743853745469645\n",
      "4.573022787625018\n",
      "4.5716629619283236\n",
      "4.570305891591707\n",
      "4.568951570762553\n",
      "4.56759999360074\n",
      "4.56625115427868\n",
      "4.564905046981238\n",
      "4.563561665905741\n",
      "4.562221005261907\n",
      "4.560883059271882\n",
      "4.559547822170133\n",
      "4.558215288203494\n",
      "4.556885451631106\n",
      "4.555558306724386\n",
      "4.554233847767019\n",
      "4.552912069054912\n",
      "4.551592964896188\n",
      "4.550276529611134\n",
      "4.548962757532213\n",
      "4.547651643003984\n",
      "4.546343180383133\n",
      "4.545037364038405\n",
      "4.543734188350596\n",
      "4.542433647712532\n",
      "4.541135736529027\n",
      "4.539840449216861\n",
      "4.538547780204776\n",
      "4.5372577239334175\n",
      "4.535970274855329\n",
      "4.534685427434928\n",
      "4.533403176148459\n",
      "4.532123515483994\n",
      "4.530846439941401\n",
      "4.529571944032304\n",
      "4.5283000222800665\n",
      "4.527030669219776\n",
      "4.525763879398209\n",
      "4.524499647373792\n",
      "4.523237967716611\n",
      "4.521978835008359\n",
      "4.520722243842314\n",
      "4.519468188823325\n",
      "4.518216664567769\n",
      "4.516967665703566\n",
      "4.515721186870094\n",
      "4.514477222718211\n",
      "4.513235767910221\n",
      "4.511996817119831\n",
      "4.510760365032157\n",
      "4.509526406343658\n",
      "4.5082949357621676\n",
      "4.507065948006807\n",
      "4.505839437808016\n",
      "4.504615399907486\n",
      "4.503393829058156\n",
      "4.502174720024199\n",
      "4.5009580675809815\n",
      "4.499743866515033\n",
      "4.498532111624039\n",
      "4.497322797716809\n",
      "4.496115919613262\n",
      "4.494911472144381\n",
      "4.493709450152213\n",
      "4.492509848489827\n",
      "4.491312662021309\n",
      "4.490117885621714\n",
      "4.488925514177069\n",
      "4.487735542584325\n",
      "4.486547965751354\n",
      "4.485362778596919\n",
      "4.484179976050633\n",
      "4.48299955305297\n",
      "4.481821504555208\n",
      "4.480645825519426\n",
      "4.479472510918476\n",
      "4.478301555735961\n",
      "4.477132954966198\n",
      "4.475966703614223\n",
      "4.474802796695755\n",
      "4.473641229237149\n",
      "4.4724819962754\n",
      "4.471325092858138\n",
      "4.4701705140435575\n",
      "4.469018254900419\n",
      "4.467868310508042\n",
      "4.46672067595625\n",
      "4.4655753463453856\n",
      "4.46443231678624\n",
      "4.463291582400082\n",
      "4.462153138318598\n",
      "4.4610169796838886\n",
      "4.4598831016484315\n",
      "4.4587514993750785\n",
      "4.457622168037018\n",
      "4.4564951028177635\n",
      "4.455370298911124\n",
      "4.454247751521169\n",
      "4.453127455862246\n",
      "4.452009407158919\n",
      "4.450893600645967\n",
      "4.449780031568336\n",
      "4.4486686951811745\n",
      "4.447559586749751\n",
      "4.446452701549463\n",
      "4.445348034865798\n",
      "4.4442455819943305\n",
      "4.44314533824071\n",
      "4.44204729892059\n",
      "4.440951459359654\n",
      "4.439857814893575\n",
      "4.4387663608680015\n",
      "4.43767709263854\n",
      "4.436590005570695\n",
      "4.435505095039922\n",
      "4.434422356431516\n",
      "4.433341785140684\n",
      "4.4322633765724415\n",
      "4.4311871261416425\n",
      "4.430113029272935\n",
      "4.429041081400767\n",
      "4.4279712779693075\n",
      "4.426903614432513\n",
      "4.425838086254028\n",
      "4.424774688907193\n",
      "4.423713417875046\n",
      "4.422654268650258\n",
      "4.4215972367351535\n",
      "4.420542317641658\n",
      "4.419489506891298\n",
      "4.4184388000151715\n",
      "4.417390192553945\n",
      "4.416343680057782\n",
      "4.415299258086392\n",
      "4.4142569222089705\n",
      "4.413216668004161\n",
      "4.412178491060094\n",
      "4.411142386974299\n",
      "4.410108351353744\n",
      "4.409076379814761\n",
      "4.40804646798308\n",
      "4.407018611493758\n",
      "4.4059928059911995\n",
      "4.404969047129116\n",
      "4.403947330570493\n",
      "4.402927651987618\n",
      "4.401910007062008\n",
      "4.400894391484413\n",
      "4.399880800954803\n",
      "4.3988692311823385\n",
      "4.397859677885341\n",
      "4.396852136791301\n",
      "4.395846603636829\n",
      "4.3948430741676665\n",
      "4.3938415441386205\n",
      "4.392842009313605\n",
      "4.391844465465566\n",
      "4.3908489083764985\n",
      "4.389855333837408\n"
     ]
    }
   ],
   "source": [
    "grad = get_gradient(A, P)\n",
    "print(np.sum(grad))\n",
    "while np.sum(np.abs(grad)) > 8:\n",
    "    A_grad = np.zeros((n , n))\n",
    "    A_grad[np.tril_indices(n)] = grad[:int(n * (n + 1) / 2)]\n",
    "    P_grad = grad[int(n * (n + 1) / 2):].reshape((n , n))\n",
    "\n",
    "    A -= eta * A_grad * 1\n",
    "    P -= eta * P_grad\n",
    "    \n",
    "    grad = get_gradient(A, P)\n",
    "    print(expected_cost(A, sigmoid(P)))    \n",
    "    \n",
    "# P = sinkhorn_balance(P)\n",
    "\n",
    "# print(A_grad)\n",
    "# print(P_grad)\n",
    "#print(A, P)\n",
    "#print(expected_cost(A, P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b656f764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.65369742 0.         0.        ]\n",
      " [0.38003372 0.3548804  0.        ]\n",
      " [0.16511378 0.25673041 0.25471694]] [[0.71642165 0.18074869 0.25404891]\n",
      " [0.47405659 0.55458431 0.69625908]\n",
      " [0.47998471 0.77333756 0.34366889]]\n"
     ]
    }
   ],
   "source": [
    "print(A,P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5791,
   "id": "dd1570a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.59965663 0.1187942  0.28154917]\n",
      " [0.15568219 0.4934595  0.35085831]\n",
      " [0.24466118 0.3877463  0.36759252]]\n"
     ]
    }
   ],
   "source": [
    "def sinkhorn_balance(P):\n",
    "    for _ in range(10000):\n",
    "        # normalize rows\n",
    "        r_sum = P.sum(axis=1)\n",
    "        P = P / r_sum[:, np.newaxis]\n",
    "        \n",
    "        \n",
    "        c_sum = P.sum(axis = 0)\n",
    "        P = P / c_sum\n",
    "    return P\n",
    "\n",
    "Ps = np.array([[0.4, 0.1, 0.5], [0.1, 0.4, 0.6], [1, 2, 4]])\n",
    "print(sinkhorn_balance(Ps))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python36864bit874e3a48d9b148faaa09714964fd179b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
