{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2386d9c1",
   "metadata": {},
   "source": [
    "### Problem Setting\n",
    "#### Original Model Description\n",
    "First, we generate data $X_1 \\in \\mathbb{R}^{T}$, $X_2 \\in \\mathbb{R}^T$ according to two AR(1) models. They are as follows:\n",
    "$$X_{1,t} = aX_{1,t-1} + \\varepsilon_{1,t}$$\n",
    "$$X_{2,t} = bX_{2,t-1} + \\varepsilon_{2,t}$$\n",
    "Here $a$ and $b$ are coefficient with absolute value smaller than 1. Furthermore, $$\\varepsilon_{i, t} \\sim \\mathcal{N}\\left(0, \\sigma_X^2\\right)\\qquad \\text{for $i= 1, 2; t = 1, \\ldots T$.}$$ For now, we initialize $$X_{i, 0} = 0\\qquad \\text{for $i = 1, 2$}.$$\n",
    "\n",
    "We now generate $Y$ as a noise measurement of $X_1$. So, $$Y_t = X_{1, t} + \\varepsilon_t,$$ where $\\varepsilon_t \\sim \\mathcal{N}(0, \\sigma_Y^2)$.\n",
    "\n",
    "#### Alternative Model\n",
    "Now, let us define the model where we can also use the second time series $X_2$. That is, $$\\hat{Y}_t = X_{1, t} + +\\hat{\\beta} X_{2, t} + \\varepsilon_t,$$ where $\\varepsilon_t \\sim \\mathcal{N}(0, \\hat{\\sigma_Y^2})$.\n",
    "\n",
    "The question now is the following:\n",
    "$$H_0: \\hat{\\beta} = 0,\\qquad H_1: \\hat{\\beta} \\neq 0.$$\n",
    "\n",
    "#### Cross-Validation\n",
    "We hope to answer this question using Cross-Validation. For this, we define $\\hat{\\beta}^{(-t)}$ as the cross-validated LS-estimator for $\\beta$, obtained by removing the $t$th element of the training set. \n",
    "$$\\hat{\\beta}^{(-t')} = \\frac{\\sum_{t = 1,t \\neq t'}^T X_{2,t} (Y_t - X_{1, t})}{\\sum_{t = 1, t \\neq t'}^T (X_{2,t})^2} = \\frac{\\sum_{t = 1, t\\neq t'}^T X_{2,t} \\varepsilon_t}{\\sum_{t = 1, t \\neq t'}^T (X_{2,t})^2}.$$\n",
    "Then, the cross-validated risk is equal to \n",
    "\n",
    "\\begin{align}CV &= \\frac{1}{T} \\sum_{t = 1}^T \\left(Y_t - X_{1, t}\\right)^2 - \\left(Y_t - X_{1, t} - \\hat{\\beta}^{(-t)} X_{2, t} \\right)^2 \\\\ &=  \\frac{1}{T}\\sum_{t = 1}^T \\left( \\varepsilon_t^2 - \\left(\\varepsilon_t - \\hat{\\beta}^{(-t)} X_{2, t}\\right) ^2\\right) \\\\ &=  \\frac{1}{T}\\sum_{t = 1}^T \\varepsilon_t^2 - \\varepsilon_t^2 + 2\\varepsilon_t\\hat{\\beta}^{(-t)} X_{2, t} - \\left(\\hat{\\beta}^{(-t)} X_{2, t}\\right)^2 \\\\ &=  \\frac{1}{T}\\sum_{t = 1}^T 2\\varepsilon_t\\hat{\\beta}^{(-t)} X_{2, t} - \\left(\\hat{\\beta}^{(-t)} X_{2, t}\\right)^2 \\\\ &=  \\frac{1}{T}\\sum_{t = 1}^T \\hat{\\beta}^{(-t)} X_{2, t}\\left(2\\varepsilon_t - \\hat{\\beta}^{(-t)} X_{2, t}\\right).\\end{align}\n",
    "\n",
    "#### Expectation\n",
    "An important note is that $\\varepsilon_t$ is independent of $X_{2}$, and $\\varepsilon_t$ also has zero mean. Therefore, if we are interested in $\\mathbb{E}[CV]$, then we know that the first term will vanish. What is left is the second term: $$-\\frac{1}{T} \\sum_{t = 1}^T\\mathbb{E}\\left[\\left(\\hat{\\beta}^{(-t)} X_{2, t}\\right)^2\\right]$$\n",
    "\n",
    "We know that \n",
    "\n",
    "\\begin{align}\\mathbb{E}\\left[\\left(\\hat{\\beta}^{(-t)} X_{2, t}\\right)^2\\right] &= \\mathbb{V}\\left(\\hat{\\beta}^{(-t)} X_{2, t}\\right) - \\mathbb{E}\\left[\\hat{\\beta}^{(-t)} X_{2, t}\\right]^2.\\end{align} Unfortunately, this is quite a difficult beast to tame, as the time series $X_{2,t}$ is dependent, therefore $\\beta^{(-t)}$ and $X_{2,t}$ are dependent.\n",
    "\n",
    "Nevertheless, we know that this quantity depends on $T$, $p$, $\\sigma_Y^2$, $b$, and $\\sigma_X^2$,\n",
    "\n",
    "Also, a remark is that we pick the correct hypothesis, i.e., $H_0$, whenever the bias is negative. Interestingly, the mean of the bias is always negative, so we are in good shape when the variability of the first term is small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12ea56b",
   "metadata": {},
   "source": [
    "$$\\mathbb{E}\\left[\\hat{\\beta}^{(-t)} X_{2, t}\\right]$$ Seems to have zero mean. Its covariance seems to decrease rapidly with T.\n",
    "\n",
    "We see that $$\\frac{1}{T}\\sum_{t = 1}^T \\mathbb{E}\\left[\\hat{\\beta}^{(-t)} X_{2, t}\\right]^2 \\approx T \\mathbb{V}\\left( \\hat{\\beta}^{(-t)} X_{2, t}\\right)$$\n",
    "For a larger $b$, we get closer to normality?\n",
    "\n",
    "For $b$ large, we seem to have that $$\\frac{1}{T}\\sum_{t = 1}^T\\hat{\\beta}^{(-t)} X_{2, t} \\sim \\mathcal{N}\\left(0, \\frac{\\sigma_Y^2}{T - 1}\\right)$$  and hence, $$-\\frac{1}{T} \\sum_{t = 1}^T\\mathbb{E}\\left[\\left(\\hat{\\beta}^{(-t)} X_{2, t}\\right)^2\\right] \\approx -\\frac{\\sigma_Y^2}{T - 1}.$$\n",
    "- Now, increasing $\\sigma_Y^2$ makes the bias even more negative, but it also increases the variance, and therefore it does not significantly affect the probability of doing the right thing.\n",
    "- Changing $\\sigma_X^2$ does not affect anything whatsoever.\n",
    "- For a small $b$, the bias seems to be even smaller, but this difference disappears for larger $T$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af71b8a4",
   "metadata": {},
   "source": [
    "#### Covariance\n",
    "A more difficult quantity is the covariance, as we have a lot of interplay between depdencies.\n",
    "\n",
    "\\begin{align}\\mathbb{V}(CV) &= \\frac{1}{T^2}\\mathbb{V}\\left[\\sum_{t = 1}^T 2\\varepsilon_t\\hat{\\beta}^{(-t)} X_{2, t} - \\left(\\hat{\\beta}^{(-t)} X_{2, t}\\right)^2\\right].\\end{align}\n",
    "\n",
    "- Changing $\\sigma_Y^2$ seems to have a linear effect on the covariance.\n",
    "- Changing $\\sigma_X^2$ seems to have no effect whatsoever.\n",
    "- Changing $T$ seems to have a quadratic effect on the covariance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python36864bit874e3a48d9b148faaa09714964fd179b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
