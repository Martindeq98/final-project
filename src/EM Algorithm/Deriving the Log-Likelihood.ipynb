{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e062e37",
   "metadata": {},
   "source": [
    "Let us consider the model $$X_{t+1} = P^{-1}APX_t + E,$$\n",
    "where $E \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma)$. Furthermore, let $X_0 = \\mathbf{0}$. Our variables $X_t$ are $p$-dimensional, and our time-horizon is $T$.\n",
    "\n",
    "We have that the negative log-likelihood $\\ln l(A, P)$ is equal to \n",
    "\n",
    "\\begin{align*}\n",
    "    -\\ln l(A, P) &= \\frac{pT}{2} + \\frac{T}{2}\\ln | \\Sigma | + \\frac{1}{2}\\sum_{t = 1}^T \\left(X_{t} - P^{-1}AP X_{t-1})\\right)^T \\Sigma^{-1} \\left(X_{t} - P^{-1}AP X_{t-1})\\right)\\\\\n",
    "    &\\propto \\sum_{t = 1}^T \\left(X_{t} - P^{-1}AP X_{t-1}\\right)^T \\Sigma^{-1} \\left(X_{t} - P^{-1}AP X_{t-1}\\right)\\\\\n",
    "    &= \\text{Tr}\\left(\\left(\\mathbf{X}_T - P^{-1}AP\\mathbf{X}_{T-1}\\right)^T\\Sigma^{-1}\\left(\\mathbf{X}_T - P^{-1}AP\\mathbf{X}_{T-1}\\right)\\right)\n",
    "\\end{align*}\n",
    "\n",
    "where $\\mathbf{X_T} = (X_1, X_2, \\cdots, X_T)^T$, and $\\mathbf{X_{T-1}} = (X_0, X_1, \\cdots, X_{T-1})^T$.\n",
    "\n",
    "[link](https://link.springer.com/content/pdf/10.1007%2F978-3-540-27752-1.pdf), [link2](https://ocw.mit.edu/courses/mathematics/18-s096-topics-in-mathematics-with-applications-in-finance-fall-2013/lecture-notes/MIT18_S096F13_lecnote11.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e439f91",
   "metadata": {},
   "source": [
    "## MLE of a VAR(1) model\n",
    "The maximum likelihood estimator for $P^{-1}AP$ is consistent with the least squares estimator, namely\n",
    "\n",
    "$$\\tilde{P}^{-1}\\tilde{A} \\tilde{P} = \\left(\\left(\\mathbf{X}_{T-1}\\mathbf{X}_{T-1}^T\\right)^{-1}X_{T-1} \\otimes I_p\\right)\\left(\\mathbf{X}_T\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e2d49a",
   "metadata": {},
   "source": [
    "## EM Algorithm\n",
    "Idea: In the $E$-step, we impute our matrix $P$. In the $M$-step, we maximize this negative log likelihood function with respect to $A$.\n",
    "\n",
    "\\begin{align*}\n",
    "Q(A\\ |\\ A^t) :&= \\mathbf{E}_{P|X, A}\\left[\\ln(A, P)\\right] \\\\\n",
    "A^{t+1} &= {\\arg \\max}_A Q(A | A^t)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0a8fa7",
   "metadata": {},
   "source": [
    "## $E$-step:\n",
    "The $E$-Step is arguably the hardest part. Often, the EM-algorithm is used to handle missing data. We do not have missing data, but an unknown parameter. This is also the case in Gaussian Mixture Models. Question: how do we do this for VAR(1) models? Literature only discusses EM for missing data in VAR.\n",
    "\n",
    "## $M$-step:\n",
    "In the $M$-step, we maximize the expectation of the log likelihood of $P$ given our data $X$ and the previous estimates of our matrix $A^t$ with respect to $A$. This is quite easy and can be done by e.g. least squares or maxmimum likelihood estimators."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python36864bit874e3a48d9b148faaa09714964fd179b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
