{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "98dbcd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is our A\n",
      " [[-3.4         0.          0.        ]\n",
      " [-3.72698541 -0.55477641  0.        ]\n",
      " [-0.16548296 -1.25400135 -0.08763634]]\n"
     ]
    }
   ],
   "source": [
    "##Readme: This notebook is here to analyze failure cases of the notears VAR approach. We first generate some data. \n",
    "#Then we normalize it to be 0-mean and 1std and run no tears. So far that has fixed the problem for all data that\n",
    "#I tried\n",
    "\n",
    "\n",
    "## Generating the data\n",
    "import numpy as np\n",
    "\n",
    "#That was my original random seed that gave me the wild data\n",
    "np.random.seed(3)\n",
    "dim = 3\n",
    "n = 20\n",
    "A = np.random.normal(0, 2, size = (dim, dim))\n",
    "A = np.tril(A, k = 0)\n",
    "\n",
    "# IMPORTANT: Here we force the only non-negative diagonal entry to be negative. This gives the wrong behavior\n",
    "A[0, 0] = -3.4\n",
    "\n",
    "# Here we can permute A if we want\n",
    "# I = np.identity(dim)\n",
    "# P = np.random.permutation(I)\n",
    "# A = P.T @ A @ P\n",
    "\n",
    "X_ini = np.zeros([n,dim])\n",
    "X_ini[0] = np.random.normal(0, 2, size=(1, dim))\n",
    "\n",
    "for k in range(n - 1):\n",
    "    X_ini[k + 1] = X_ini[k] @ A #+np.random.normal(0, 0.5, size=(1, dim)) #I start with the no noise case\n",
    "    \n",
    "print('This is our A\\n', A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e6715fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is our A after normalizing\n",
      " [[-3.40000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [-2.31551006e-10 -5.54776405e-01  0.00000000e+00]\n",
      " [-6.48305992e-12 -7.90743759e-01 -8.76363380e-02]\n",
      " [-7.36752155e-01  2.13131794e-01 -2.27570025e-01]]\n"
     ]
    }
   ],
   "source": [
    "#We normalize the data to be 0 mean and 1std\n",
    "\n",
    "N1 = X_ini.std(axis=0)\n",
    "N2 = X_ini.mean(axis=0)\n",
    "X = (X_ini - N2) / N1\n",
    "\n",
    "# Doing this we also have the change the true A. First, we have to add a bias, as we add a constant to our feature X,\n",
    "# we now have an affine model X_t=X_{t-1}@A+C. We can model that by adding a row to A (called bias here) and add \n",
    "# a constant 1 column to X. We will have to do changes accordingly in the notears algorithm\n",
    "\n",
    "bias = N2.reshape(1, 3) / N1 - ((N2 @ A).T / N1).T #looks a bit wild, but it's really just going through the motions\n",
    "\n",
    "# Entries of A change as well\n",
    "A = A.T * N1\n",
    "A = A.T / N1\n",
    "\n",
    "# As said before, we have to add the constant 1 to actually take the bias into account\n",
    "# I will for now implement that implicitly in the notears by checking if we have that extra dimension, so it\n",
    "# works with affine and linear models\n",
    "\n",
    "X = np.concatenate((X, np.ones((n, 1))), axis = 1)\n",
    "A = np.concatenate((A, -bias),axis=0)\n",
    "\n",
    "print('This is our A after normalizing\\n', A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "05c9d06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.16639254  0.24344874 -0.20923356  1.        ] [ 4.16639254  0.24344874 -0.20923356]\n"
     ]
    }
   ],
   "source": [
    "#Checking that we actually changed A correctly\n",
    "t = 18\n",
    "print(X[t+1], X[t] @ A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a71869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is just to initialize all functions for some error analysis later, no need to read it.\n",
    "n, d = X.shape\n",
    "rho, alpha, h =  1.0, 0.0, np.inf  # double w_est into (w_pos, w_neg)\n",
    "\n",
    "lambda1 = 0\n",
    "loss_type = 'l2'\n",
    "\n",
    "bnds = [ (0, None) for _ in range(2) for i in range(d) for j in range(d)]\n",
    "def _loss(W):\n",
    "    \"\"\"Evaluate value and gradient of loss.\"\"\"\n",
    "    X1=np.delete(X,-1,0)\n",
    "    X2=np.delete(X[:,:-1],0,0)\n",
    "    M = X1 @ W\n",
    "    if loss_type == 'l2':\n",
    "        R = X2 - M\n",
    "        loss = 0.5 / X1.shape[0] * (R ** 2).sum()\n",
    "        G_loss = - 1.0 / X1.shape[0] * X1.T @ R\n",
    "    #Not changed yet, anyway only works for binary input, not relevant\n",
    "    elif loss_type == 'logistic':\n",
    "        loss = 1.0 / X1.shape[0] * (np.logaddexp(0, M) - X * M).sum()\n",
    "        G_loss = 1.0 / X.shape[0] * X.T @ (sigmoid(M) - X)\n",
    "    #Also not changed, not sure what poisson loss is\n",
    "    elif loss_type == 'poisson':\n",
    "        S = np.exp(M)\n",
    "        loss = 1.0 / X.shape[0] * (S - X * M).sum()\n",
    "        G_loss = 1.0 / X.shape[0] * X.T @ (S - X)\n",
    "    else:\n",
    "        raise ValueError('unknown loss type')\n",
    "    return loss, G_loss\n",
    "\n",
    "def _h(W):\n",
    "    \"\"\"Evaluate value and gradient of acyclicity constraint.\"\"\"\n",
    "    W=W[:-1,:]\n",
    "    V=W * W-np.diag(np.diag(W * W))\n",
    "    E = slin.expm(V)  # (Zheng et al. 2018)\n",
    "    h = np.trace(E) -d# -np.trace(W*W)\n",
    "    #     # A different formulation, slightly faster at the cost of numerical stability\n",
    "    #     M = np.eye(d) + W * W / d  # (Yu et al. 2019)\n",
    "    #     E = np.linalg.matrix_power(M, d - 1)\n",
    "    #     h = (E.T * M).sum() - d\n",
    "    G_h = E.T * V * 2#-2*np.diag(np.diag(W))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return h, G_h\n",
    "\n",
    "def _adj(w):\n",
    "    \"\"\"Convert doubled variables ([2 d^2] array) back to original variables ([d, d] matrix).\"\"\"\n",
    "    return (w[:(d-1)**2+d-1] - w[(d-1)**2+d-1:]).reshape([d, d-1])\n",
    "\n",
    "def _func(w):\n",
    "    \"\"\"Evaluate value and gradient of augmented Lagrangian for doubled variables ([2 d^2] array).\"\"\"\n",
    "    W = _adj(w)\n",
    "    loss, G_loss = _loss(W)\n",
    "    h, G_h = _h(W)\n",
    "    G_h=np.concatenate((G_h,np.zeros((1,d-1))),axis=0)\n",
    "    obj = loss + 0.5 * rho * h * h + alpha * h + lambda1 *(w[:(d-1)**2].sum()+w[(d-1)**2+d-1:2*(d-1)**2+d-1].sum())\n",
    "    G_smooth = G_loss + (rho * h + alpha) * G_h\n",
    "    g_obj = np.concatenate((G_smooth + lambda1, - G_smooth + lambda1), axis=None)\n",
    "    return obj, g_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "068eca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"C:/Users/s165048/OneDrive - TU Eindhoven/QuinceyFinalProject/final-project/src/Week 12/notears/notears\"\n",
    "os.chdir(path)\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6c06a79d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9999999011250074\n",
      "-0.12265458183390532\n",
      "-0.12265458183390532\n",
      "-0.12265458183390532\n",
      "-0.12265458183390532\n",
      "-0.12265458183390532\n",
      "-0.12265458183390532\n",
      "-0.12265458183390532\n",
      "-0.12265458183390532\n",
      "-0.12265458183390532\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "## This actually runs the optimization. NOTE: This now only works for the affine model, so X needs the extra column of ones.\n",
    "import numpy as np\n",
    "import scipy.linalg as slin\n",
    "import scipy.optimize as sopt\n",
    "import argparse\n",
    "import warnings\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "\n",
    "def notears_linear(X, lambda1, loss_type, max_iter=10, h_tol=1e-8, rho_max=1e+16, w_threshold=0.0):\n",
    "    \"\"\"Solve min_W L(W; X) + lambda1 ‖W‖_1 s.t. h(W) = 0 using augmented Lagrangian.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): [n, d] sample matrix\n",
    "        lambda1 (float): l1 penalty parameter\n",
    "        loss_type (str): l2, logistic, poisson\n",
    "        max_iter (int): max num of dual ascent steps\n",
    "        h_tol (float): exit if |h(w_est)| <= htol\n",
    "        rho_max (float): exit if rho >= rho_max\n",
    "        w_threshold (float): drop edge if |weight| < threshold\n",
    "\n",
    "    Returns:\n",
    "        W_est (np.ndarray): [d, d] estimated DAG\n",
    "    \"\"\"\n",
    "    def _loss(W):\n",
    "        \"\"\"Evaluate value and gradient of loss.\"\"\"\n",
    "        X1=np.delete(X,-1,0)\n",
    "        X2=np.delete(X[:,:-1],0,0)\n",
    "        M = X1 @ W\n",
    "        if loss_type == 'l2':\n",
    "            R = X2 - M\n",
    "            loss = 0.5 / X1.shape[0] * (R ** 2).sum()\n",
    "            G_loss = - 1.0 / X1.shape[0] * X1.T @ R\n",
    "        #Not changed yet, anyway only works for binary input, not relevant\n",
    "        elif loss_type == 'logistic':\n",
    "            loss = 1.0 / X1.shape[0] * (np.logaddexp(0, M) - X * M).sum()\n",
    "            G_loss = 1.0 / X.shape[0] * X.T @ (sigmoid(M) - X)\n",
    "        #Also not changed, not sure what poisson loss is\n",
    "        elif loss_type == 'poisson':\n",
    "            S = np.exp(M)\n",
    "            loss = 1.0 / X.shape[0] * (S - X * M).sum()\n",
    "            G_loss = 1.0 / X.shape[0] * X.T @ (S - X)\n",
    "        else:\n",
    "            raise ValueError('unknown loss type')\n",
    "        return loss, G_loss\n",
    "\n",
    "    def _h(W):\n",
    "        \"\"\"Evaluate value and gradient of acyclicity constraint.\"\"\"\n",
    "        W=W[:-1,:]\n",
    "        V=W * W-np.diag(np.diag(W * W))\n",
    "        E = slin.expm(V)  # (Zheng et al. 2018)\n",
    "        h = np.trace(E) -d# -np.trace(W*W)\n",
    "        #     # A different formulation, slightly faster at the cost of numerical stability\n",
    "        #     M = np.eye(d) + W * W / d  # (Yu et al. 2019)\n",
    "        #     E = np.linalg.matrix_power(M, d - 1)\n",
    "        #     h = (E.T * M).sum() - d\n",
    "        G_h = E.T * V * 2#-2*np.diag(np.diag(W))\n",
    "\n",
    "      \n",
    "  \n",
    " \n",
    "        \n",
    "        return h, G_h\n",
    "\n",
    "    def _adj(w):\n",
    "        \"\"\"Convert doubled variables ([2 d^2] array) back to original variables ([d, d] matrix).\"\"\"\n",
    "        return (w[:(d-1)**2+d-1] - w[(d-1)**2+d-1:]).reshape([d, d-1])\n",
    "\n",
    "    def _func(w):\n",
    "        \"\"\"Evaluate value and gradient of augmented Lagrangian for doubled variables ([2 d^2] array).\"\"\"\n",
    "        W = _adj(w)\n",
    "        loss, G_loss = _loss(W)\n",
    "        h, G_h = _h(W)\n",
    "        G_h=np.concatenate((G_h,np.zeros((1,d-1))),axis=0)\n",
    "        obj = loss + 0.5 * rho * h * h + alpha * h + lambda1 *(w[:(d-1)**2].sum()+w[(d-1)**2+d-1:2*(d-1)**2+d-1].sum())\n",
    "        G_smooth = G_loss + (rho * h + alpha) * G_h\n",
    "        g_obj = np.concatenate((G_smooth + lambda1, - G_smooth + lambda1), axis=None)\n",
    "        return obj, g_obj\n",
    "\n",
    "    n, d = X.shape\n",
    "    w_est, rho, alpha, h = np.zeros(2 * ((d-1)**2+d-1)), 1.0, 0.0, np.inf  # double w_est into (w_pos, w_neg)\n",
    "    #bnds = [ (0, None) for _ in range(2) for i in range(d) for j in range(d)]\n",
    "    for _ in range(max_iter):\n",
    "        w_new, h_new = None, None\n",
    "        while rho < rho_max:\n",
    "            sol = sopt.minimize(_func, w_est, method='L-BFGS-B', jac=True )\n",
    "            w_new = sol.x\n",
    "            h_new, _ = _h(_adj(w_new))\n",
    "            w_est, h = w_new, h_new\n",
    "            alpha += rho * h\n",
    "            if h_new > 0.25 * h:\n",
    "                rho *= 10\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        print(h)\n",
    "        if np.abs(h) <= h_tol or rho >= rho_max:\n",
    "            break\n",
    "    W_est = _adj(w_est)\n",
    "    W_est[np.abs(W_est) < w_threshold] = 0\n",
    "    return W_est,w_est\n",
    "\n",
    "def get_input():\n",
    "    \"\"\"Reads an CSV into the data matrix X\"\"\"\n",
    "    input = argparse.ArgumentParser(description='Reads CSV into data matrix X')\n",
    "    input.add_argument('--X', default=None, help='Pass a CSV file that corresponds to input data X')\n",
    "    args = input.parse_args()\n",
    "    return np.genfromtxt(args.X+'.csv',delimiter=',')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #warnings.simplefilter('error')\n",
    "    import utils\n",
    "    utils.set_random_seed(1)\n",
    "    ## Uncomment the following to run the example\n",
    "    # n, d, s0, graph_type, sem_type = 100, 20, 20, 'ER', 'gauss'\n",
    "    # B_true = utils.simulate_dag(d, s0, graph_type)\n",
    "    # W_true = utils.simulate_parameter(B_true)\n",
    "    # np.savetxt('W_true.csv', W_true, delimiter=',')\n",
    "\n",
    "    # X = utils.simulate_linear_sem(W_true, n, sem_type)\n",
    "    # np.savetxt('X.csv', X, delimiter=',')\n",
    "    W_est,w_est = notears_linear(X, lambda1=0, loss_type='l2')\n",
    "    #assert utils.is_dag(W_est)\n",
    "    #B_true=np.genfromtxt('W_est.csv',delimiter=',')\n",
    "    print('DONE')\n",
    "    #np.savetxt('W_est.csv', W_est.T, delimiter=',')\n",
    " \n",
    "    #acc = utils.count_accuracy(B_true, W_est != 0)\n",
    "    #print(acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "64174e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True matrix\n",
      " [[-3.40000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [-2.31551006e-10 -5.54776405e-01  0.00000000e+00]\n",
      " [-6.48305992e-12 -7.90743759e-01 -8.76363380e-02]\n",
      " [-7.36752155e-01  2.13131794e-01 -2.27570025e-01]]\n",
      "Estimated matrix\n",
      " [[-3.3103295  -0.0697375  -0.116879  ]\n",
      " [ 0.04477571 -0.59422257 -0.92507845]\n",
      " [-0.35883158 -0.97753726 -1.04441649]\n",
      " [-0.74285576  0.05889454 -0.12959465]]\n",
      "Value of true matrix 0.5 Value of estimated matrix 0.2228804268653325\n",
      "Gradient of w_est [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Loss of true matrix 1.2229655146858948e-32 Loss of estimated matrix 0.2153583536429074\n"
     ]
    }
   ],
   "source": [
    "print('True matrix\\n',A)\n",
    "print('Estimated matrix\\n',W_est)\n",
    "B=A.reshape(((d-1)**2+d-1,))\n",
    "C=np.concatenate((B.clip(min=0),-B.clip(max=0)),axis=0)\n",
    "#Computing actual objective of the values\n",
    "val1,_=_func(C)\n",
    "val2,_=_func(w_est)\n",
    "print('Value of true matrix',val1,'Value of estimated matrix',val2)\n",
    "#We see that the actual matrix achieves a much lower value in the objective, so ideally the algorithm would find it\n",
    "#So, instead is W_est a local minimum, let's check the gradient of the objective\n",
    "_,grad=_func(w_est)\n",
    "#Gradient\n",
    "print('Gradient of w_est',grad[:(d-1)**2]+grad[(d-1)**2+d-1:2*(d-1)**2+d-1])\n",
    "print('Loss of true matrix',_loss(A)[0],'Loss of estimated matrix',_loss(W_est)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adb72fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
